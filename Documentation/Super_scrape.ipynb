{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 20:08:57.911958: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-07 20:08:57.953992: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from pytube import YouTube\n",
    "from moviepy.editor import VideoFileClip\n",
    "import deepface\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import yt_dlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function checks if the directories for each JSON file's output (test, train, and validation sets) exist. If they do not exist, it creates them.This organization is essential for storing the processed video segments in separate folders based on their JSON source, facilitating data management and retrieval.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: ./MSAL_test_output\n",
      "Directory already exists: ./MSASL_train_output\n",
      "Directory already exists: ./MSASL_val_output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define paths for each JSON file's output directories\n",
    "# These paths will store the processed video segments for each JSON file\n",
    "BASE_PATHS = {\n",
    "    \"test\": \"./MSAL_test_output\",      # Output folder for MSAL_test JSON data\n",
    "    \"train\": \"./MSASL_train_output\",   # Output folder for MSASL_train JSON data\n",
    "    \"val\": \"./MSASL_val_output\"        # Output folder for MSASL_val JSON data\n",
    "}\n",
    "\n",
    "def setup_output_folders():\n",
    "\n",
    "    for key, path in BASE_PATHS.items():\n",
    "        if not os.path.exists(path):\n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(path)\n",
    "            print(f\"Created directory: {path}\")\n",
    "        else:\n",
    "            # If directory exists, notify that it's already present\n",
    "            print(f\"Directory already exists: {path}\")\n",
    "\n",
    "# Call the function to create folders\n",
    "setup_output_folders()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrects the YouTube URL format if necessary, downloads the video using pytube,\n",
    "    and saves it to the corresponding folder based on dataset type (test, train, or val).\n",
    "    \n",
    "    Parameters:\n",
    "    - url (str): The original URL of the YouTube video.\n",
    "    - dataset_type (str): The dataset category, which determines the output folder \n",
    "                          (e.g., 'test', 'train', or 'val').\n",
    "\n",
    "    Returns:\n",
    "    - file_path (str): The path where the downloaded video is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp as youtube_dl\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Define base paths for output\n",
    "BASE_PATHS = {\n",
    "    \"test\": \"./MSAL_test_output\",\n",
    "    \"train\": \"./MSASL_train_output\",\n",
    "    \"val\": \"./MSASL_val_output\"\n",
    "}\n",
    "\n",
    "def correct_and_download_video_yt_dlp(url, dataset_type):\n",
    "    \"\"\"\n",
    "    Downloads the video from YouTube using yt-dlp. Skips private or unavailable videos\n",
    "    and logs any errors encountered.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define output path for yt-dlp\n",
    "        output_path = os.path.join(BASE_PATHS[dataset_type], \"%(id)s.%(ext)s\")\n",
    "        \n",
    "        # yt-dlp options, configured to ignore minor errors and avoid retries on private videos\n",
    "        ydl_opts = {\n",
    "            'format': 'bestvideo+bestaudio/best',\n",
    "            'outtmpl': output_path,\n",
    "            'quiet': True,\n",
    "            'noplaylist': True,\n",
    "            'ignoreerrors': True,  # Ignores minor errors, like permissions issues\n",
    "            'no_warnings': True,  # Suppresses most warnings from yt-dlp\n",
    "        }\n",
    "        \n",
    "        with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "            # Attempt to download the video\n",
    "            ydl.download([url])\n",
    "\n",
    "        # Construct and return the expected path of the downloaded file\n",
    "        video_id = url.split('=')[-1]\n",
    "        video_path = os.path.join(BASE_PATHS[dataset_type], f\"{video_id}.mp4\")\n",
    "        \n",
    "        # Verify if the video actually downloaded successfully\n",
    "        if os.path.exists(video_path):\n",
    "            return video_path\n",
    "        else:\n",
    "            # Log and return None if the file is not found after download attempt\n",
    "            logging.error(f\"Download failed or video file not found for URL: {url}\")\n",
    "            return None\n",
    "\n",
    "    except youtube_dl.DownloadError as e:\n",
    "        # Log the specific error and print it to the console\n",
    "        error_message = f\"Error downloading video from URL: {url}. Error: {e}\"\n",
    "        logging.error(error_message)\n",
    "        print(error_message)\n",
    "        return None  # Return None if the download fails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Slicing the Video Based on Start and End Times\n",
    "Now that we’ve successfully downloaded the videos, our next task is to slice these videos into segments based on the start and end times specified in the JSON files. Each segment will correspond to a specific American Sign Language (ASL) gesture.\n",
    "\n",
    "Purpose of Video Slicing\n",
    "The JSON file provides timing information for each ASL gesture within the YouTube video, allowing us to create individual video clips focusing on each gesture. This step is essential for isolating gestures for further processing, such as facial sentiment analysis and hand gesture detection.\n",
    "\n",
    "Approach\n",
    "Identify Start and End Times:\n",
    "Extract the start_time and end_time values from the JSON file for each entry. These values denote the time range for each ASL gesture within the downloaded video.\n",
    "Extract Video Segment:\n",
    "Using the moviepy library, open the downloaded video file and extract the segment that matches the start_time and end_time values.\n",
    "Save Each Segment:\n",
    "Save the extracted video segment locally in the appropriate output folder (test, train, or val) based on the dataset type, with each file named by its class (clean_text).\n",
    "Output Format\n",
    "Each sliced video segment will be saved in the corresponding output folder (e.g., ./MSAL_test_output) with a filename corresponding to the class name of the gesture (e.g., forget.mp4 for a segment representing the \"forget\" gesture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "\n",
    "def slice_video(video_path, start_time, end_time, label, dataset_type):\n",
    "    \"\"\"\n",
    "    Extracts a video segment based on the provided start and end times and saves it\n",
    "    as a new file named after the gesture class.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path (str): Path to the downloaded video file.\n",
    "    - start_time (float): Start time (in seconds) for the video segment.\n",
    "    - end_time (float): End time (in seconds) for the video segment.\n",
    "    - label (str): Class name for the gesture, used to name the output file.\n",
    "    - dataset_type (str): The dataset category ('test', 'train', or 'val'), which determines the output folder.\n",
    "\n",
    "    Returns:\n",
    "    - output_path (str): The path where the sliced video segment is saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the video file using moviepy\n",
    "        clip = VideoFileClip(video_path)\n",
    "        \n",
    "        # Extract the subclip based on start and end times\n",
    "        segment = clip.subclip(start_time, end_time)\n",
    "        \n",
    "        # Define the output path for the segment, using the gesture class name as the filename\n",
    "        output_path = os.path.join(BASE_PATHS[dataset_type], f\"{label}.mp4\")\n",
    "        \n",
    "        # Save the video segment to the specified output path\n",
    "        segment.write_videofile(output_path, codec=\"libx264\")\n",
    "        \n",
    "        # Close the clip after saving to free up resources\n",
    "        clip.close()\n",
    "        print(f\"Saved video segment for '{label}' from {start_time} to {end_time} seconds at {output_path}\")\n",
    "        \n",
    "        return output_path  # Return the path of the sliced segment\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error slicing video for '{label}' from {start_time} to {end_time}. Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we’ll analyze the facial expressions in each sliced video segment to determine the sentiment (e.g., happy, sad, neutral). This sentiment information will be used in the next step to rename the video files with both the class and the detected sentiment, such as \"beer-happy.mp4\".\n",
    "\n",
    "Purpose of Facial Sentiment Analysis\n",
    "This analysis allows us to add contextual information to each ASL gesture by associating a sentiment, which could be valuable for understanding the emotional context of each gesture in training data.\n",
    "\n",
    "Approach\n",
    "Extract Frames: Load the video segment and extract frames. We don’t need every frame for sentiment analysis, so analyzing every nth frame (e.g., every second frame) is usually sufficient.\n",
    "\n",
    "Analyze Each Frame: For each selected frame, we’ll use a facial sentiment analysis model to detect emotions. We’ll use the DeepFace library, which provides pre-trained models for this purpose.\n",
    "\n",
    "Determine Dominant Sentiment: Analyze the detected emotions and choose the dominant one. This could be done by calculating the most frequently detected emotion across the sampled frames.\n",
    "\n",
    "Return Sentiment Label: The dominant emotion is returned as the overall sentiment for that video segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "\n",
    "def facial_sentiment_analysis(video_path):\n",
    "    \"\"\"\n",
    "    Performs facial sentiment analysis on a video segment by sampling frames\n",
    "    and determining the dominant emotion across frames.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path (str): Path to the video segment file.\n",
    "\n",
    "    Returns:\n",
    "    - sentiment (str): The dominant emotion detected in the video segment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize OpenCV's video capture for the input video segment\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        # List to store detected emotions from sampled frames\n",
    "        emotions = []\n",
    "\n",
    "        # Frame sampling rate (analyzing every nth frame to reduce computation)\n",
    "        sampling_rate = 10  # Analyze every 10th frame for efficiency\n",
    "        frame_count = 0\n",
    "\n",
    "        # Loop through video frames\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # Break if no more frames to read\n",
    "\n",
    "            # Only analyze every nth frame (based on sampling rate)\n",
    "            if frame_count % sampling_rate == 0:\n",
    "                # Convert frame to RGB (DeepFace expects RGB format)\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Perform emotion analysis using DeepFace\n",
    "                analysis = DeepFace.analyze(rgb_frame, actions=['emotion'], enforce_detection=False)\n",
    "                \n",
    "                # Check for list structure and extract dominant emotion\n",
    "                if isinstance(analysis, list):\n",
    "                    # Iterate over detected faces and add each dominant emotion\n",
    "                    for face_data in analysis:\n",
    "                        if 'dominant_emotion' in face_data:\n",
    "                            emotions.append(face_data['dominant_emotion'])\n",
    "                elif 'dominant_emotion' in analysis:\n",
    "                    emotions.append(analysis['dominant_emotion'])\n",
    "\n",
    "            frame_count += 1  # Increment frame count\n",
    "\n",
    "        # Release the video capture object\n",
    "        cap.release()\n",
    "        \n",
    "        # Determine the most frequent emotion as the dominant sentiment\n",
    "        if emotions:\n",
    "            sentiment = max(set(emotions), key=emotions.count)\n",
    "            print(f\"Dominant sentiment for {video_path}: {sentiment}\")\n",
    "            return sentiment\n",
    "        else:\n",
    "            print(\"No emotion detected in video segment.\")\n",
    "            return \"neutral\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error performing facial sentiment analysis on {video_path}. Error: {e}\")\n",
    "        return \"neutral\"  # Default to neutral if analysis fails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Step: Renaming the File with Class and Sentiment\n",
    "Now that we have the dominant sentiment for each video segment, the next step is to rename the segment files to include both the class and sentiment, following the format class-sentiment.mp4 (e.g., beer-happy.mp4).\n",
    "\n",
    "Approach\n",
    "Define File Naming Convention:\n",
    "Use the gesture class (clean_text) and the detected sentiment to create the filename in the format class-sentiment.mp4.\n",
    "Rename the File:\n",
    "Save the renamed file in the appropriate output directory (test, train, or val), replacing the original filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_with_sentiment(file_path, label, sentiment, dataset_type):\n",
    "    \"\"\"\n",
    "    Renames the video segment file to include both the class and sentiment.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): The original file path of the video segment.\n",
    "    - label (str): Class name for the gesture.\n",
    "    - sentiment (str): Detected sentiment for the video segment.\n",
    "    - dataset_type (str): The dataset category (e.g., 'test', 'train', or 'val').\n",
    "    \n",
    "    Returns:\n",
    "    - new_path (str): The new path of the renamed file.\n",
    "    \"\"\"\n",
    "    # Define new filename based on class and sentiment\n",
    "    new_name = f\"{label}-{sentiment}.mp4\"\n",
    "    new_path = os.path.join(BASE_PATHS[dataset_type], new_name)\n",
    "    \n",
    "    try:\n",
    "        # Rename the file to the new name with sentiment included\n",
    "        os.rename(file_path, new_path)\n",
    "        print(f\"Renamed file to {new_path}\")\n",
    "        return new_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error renaming file {file_path} to {new_path}. Error: {e}\")\n",
    "        return file_path  # Return original path if renaming fails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Extracting Hand Gestures with MediaPipe\n",
    "The final step is to extract hand gestures from each video segment. We’ll use MediaPipe to detect and visualize hand landmarks, creating a simplified \"stick-man\" style representation of the hand gestures. This will help reduce the file size by focusing only on the hand movements, saving space.\n",
    "\n",
    "Approach\n",
    "Initialize MediaPipe Hand Detection:\n",
    "Set up MediaPipe’s hand detection model with necessary configurations (e.g., detection confidence).\n",
    "Process Each Frame:\n",
    "For each frame in the video, detect hand landmarks.\n",
    "Draw hand landmarks and connections as a \"stick-man\" representation.\n",
    "Save the Processed Video:\n",
    "Save the output with the same naming convention (class-sentiment.mp4) in the designated folder (e.g., MSAL_test_output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "def extract_hand_gesture(video_path, label, sentiment, dataset_type):\n",
    "    \"\"\"\n",
    "    Extracts hand gesture landmarks from a video segment, creates a stick-man\n",
    "    representation, and saves it in a simplified format to reduce storage space.\n",
    "    \n",
    "    Parameters:\n",
    "    - video_path (str): Path to the video segment file.\n",
    "    - label (str): Class name for the gesture.\n",
    "    - sentiment (str): Detected sentiment for the video segment.\n",
    "    - dataset_type (str): The dataset category (e.g., 'test', 'train', or 'val').\n",
    "    \n",
    "    Returns:\n",
    "    - output_path (str): Path where the processed hand gesture video is saved.\n",
    "    \"\"\"\n",
    "    # Define MediaPipe Hands solution\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "    \n",
    "    # Set up OpenCV video capture for reading frames from the original video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Define output file path with the naming convention\n",
    "    output_name = f\"{label}-{sentiment}.mp4\"\n",
    "    output_path = os.path.join(BASE_PATHS[dataset_type], output_name)\n",
    "    \n",
    "    # Set up video writer for the stick-man output\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "    \n",
    "    # Process each frame\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB as required by MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(frame_rgb)\n",
    "        \n",
    "        # Draw landmarks if hands are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks and connections on the frame\n",
    "                mp.solutions.drawing_utils.draw_landmarks(\n",
    "                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Write the processed frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    hands.close()\n",
    "    print(f\"Hand gesture video saved at: {output_path}\")\n",
    "    \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deprecated Feature: Support for Python version 3.8 has been deprecated. Please update to Python 3.9 or above\n",
      "Deprecated Feature: Support for Python version 3.8 has been deprecated. Please update to Python 3.9 or above\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ready: ./MSAL_test_output\n",
      "Directory ready: ./MSASL_train_output\n",
      "Directory ready: ./MSASL_val_output\n",
      "[download]  17.0% of  117.54MiB at   29.29MiB/s ETA 00:03  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Cannot update utime of file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 'milk' from 29.07 to 30.709 in dataset 'test'\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Cannot update utime of file\n",
      "Deprecated Feature: Support for Python version 3.8 has been deprecated. Please update to Python 3.9 or above\n",
      "Deprecated Feature: Support for Python version 3.8 has been deprecated. Please update to Python 3.9 or above\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 'bald' from 44.678 to 48.348 in dataset 'test'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube] cdl5N710d28: Video unavailable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing entry 'nurse': Downloaded video not found for URL: https://www.youtube.com/watch?v=cdl5N710d28\n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Cannot update utime of file\n",
      "Deprecated Feature: Support for Python version 3.8 has been deprecated. Please update to Python 3.9 or above\n",
      "Deprecated Feature: Support for Python version 3.8 has been deprecated. Please update to Python 3.9 or above\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 'believe' from 21.321 to 25.526 in dataset 'train'\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Cannot update utime of file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing entry 'help': Downloaded video not found for URL: https://www.youtube.com/watch?v=bX1eJjB3nyA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Cannot update utime of file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing entry 'airplane': Downloaded video not found for URL: https://www.youtube.com/watch?v=gVUwBsPOUjk\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "import yt_dlp\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=\"error_log.txt\", level=logging.ERROR, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Define paths for each JSON file's output directories\n",
    "BASE_PATHS = {\n",
    "    \"test\": \"./MSAL_test_output\",\n",
    "    \"train\": \"./MSASL_train_output\",\n",
    "    \"val\": \"./MSASL_val_output\"\n",
    "}\n",
    "\n",
    "# Setup output folders\n",
    "def setup_output_folders():\n",
    "    for path in BASE_PATHS.values():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        print(f\"Directory ready: {path}\")\n",
    "\n",
    "def correct_and_download_video_yt_dlp(url, dataset_type):\n",
    "    \"\"\"Download video using yt-dlp.\"\"\"\n",
    "    try:\n",
    "        output_path = os.path.join(BASE_PATHS[dataset_type], \"%(id)s.%(ext)s\")\n",
    "        ydl_opts = {'format': 'bestvideo+bestaudio/best', 'outtmpl': output_path, 'quiet': True, 'noplaylist': True}\n",
    "        \n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([url])\n",
    "\n",
    "        video_id = url.split('=')[-1]\n",
    "        return os.path.join(BASE_PATHS[dataset_type], f\"{video_id}.mp4\")\n",
    "    except yt_dlp.DownloadError as e:\n",
    "        logging.error(f\"Download error for URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_single_entry(entry, dataset_type):\n",
    "    \"\"\"Process a single JSON entry.\"\"\"\n",
    "    try:\n",
    "        url = entry.get(\"url\")\n",
    "        video_path = correct_and_download_video_yt_dlp(url, dataset_type)\n",
    "        \n",
    "        if not video_path or not os.path.exists(video_path):\n",
    "            raise FileNotFoundError(f\"Downloaded video not found for URL: {url}\")\n",
    "\n",
    "        start_time = entry.get(\"start_time\")\n",
    "        end_time = entry.get(\"end_time\")\n",
    "        label = entry.get(\"clean_text\")\n",
    "        # Slicing, sentiment, renaming, etc., would be invoked here\n",
    "        # Assume all other functions (e.g., slicing) are defined similarly\n",
    "        # ...\n",
    "\n",
    "        # Log successful processing\n",
    "        print(f\"Processed '{label}' from {start_time} to {end_time} in dataset '{dataset_type}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing entry '{entry.get('clean_text')}' in dataset '{dataset_type}': {e}\")\n",
    "        print(f\"Error processing entry '{entry.get('clean_text')}': {e}\")\n",
    "\n",
    "def process_asl_dataset_parallel(json_file_path, dataset_type):\n",
    "    \"\"\"Process JSON entries in parallel.\"\"\"\n",
    "    if not os.path.isfile(json_file_path):\n",
    "        logging.error(f\"JSON file not found: {json_file_path}\")\n",
    "        return\n",
    "\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Use random sampling for testing\n",
    "    sample_data = random.sample(data, min(2, len(data)))\n",
    "\n",
    "    # Process each entry in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_single_entry, entry, dataset_type) for entry in sample_data]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()  # Check for exceptions in threads\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    setup_output_folders()\n",
    "    # Update paths with actual locations for WSL compatibility if needed\n",
    "    process_asl_dataset_parallel(\"/mnt/d/ASL/Dataset/Json/MSASL_test.json\", \"test\")\n",
    "    process_asl_dataset_parallel(\"/mnt/d/ASL/Dataset/Json/MSASL_train.json\", \"train\")\n",
    "    process_asl_dataset_parallel(\"/mnt/d/ASL/Dataset/Json/MSASL_val.json\", \"val\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
