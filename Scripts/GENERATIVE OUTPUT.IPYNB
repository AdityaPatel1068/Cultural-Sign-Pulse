{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device: Quadro RTX 4000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Define model ID\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Define the device (ensure GPU is used if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the BitsAndBytesConfig for 8-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Allow CPU offloading for 32-bit layers\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "\n",
    "# Load the model with 8-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,  # Pass the quantization configuration\n",
    "    device_map=\"auto\",  # Automatically map layers to devices\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_llama(sentiment, sentence):\n",
    "    # Efficient prompt formatting\n",
    "    prompt = f\"Rephrase the following sentence with a {sentiment} tone: '{sentence}'.\"\n",
    "\n",
    "\n",
    "    # Tokenize input and move to GPU\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate response with optimized parameters\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=50,           # Restrict length for faster inference\n",
    "        num_beams=1,             # No beam search, single best prediction\n",
    "        do_sample=True,          # Sampling for faster results\n",
    "        top_p=0.9,               # Nucleus sampling\n",
    "        temperature=0.7          # Balance randomness and coherence\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Rephrase the following sentence with a aNGRY tone: '['LET ME GO']'.\n",
      " Unterscheidung between regular tone and angry tone is important because it can change meaningfulness of sentence dramatically transform meaningfulness of sentence dram\n"
     ]
    }
   ],
   "source": [
    "# Mock inputs\n",
    "sentiment = \"aNGRY\"\n",
    "mock_words = [\"LET ME GO\"]\n",
    "\n",
    "# Get and print the response\n",
    "response = chat_with_llama(sentiment, mock_words)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Transform the sentence '' into a angry tone. hopefully someone else can help answer this questionnaire questions answers example sentences punctu\n"
     ]
    }
   ],
   "source": [
    "def chat_with_llama(sentiment, sentence):\n",
    "    # Refined prompt\n",
    "    prompt = f\"Transform the sentence '{sentence}' into a {sentiment.lower()} tone.\"\n",
    "\n",
    "    # Tokenize input and move to GPU\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate response with stricter constraints\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=30,           # Restrict length further\n",
    "        num_beams=1,             # Single beam for speed\n",
    "        do_sample=True,          # Sampling for diversity\n",
    "        top_p=0.9,               # Nucleus sampling\n",
    "        top_k=50,                # Restrict token sampling\n",
    "        temperature=0.7,         # Balanced randomness\n",
    "        repetition_penalty=2.0   # Strong penalty for repetition\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Remove echoes of the input sentence or prompt\n",
    "    if sentence in response:\n",
    "        response = response.replace(sentence, \"\").strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Mock inputs for testing\n",
    "sentiment = \"angry\"\n",
    "mock_sentence = \"Let me go\"\n",
    "\n",
    "# Generate and print response\n",
    "response = chat_with_llama(sentiment, mock_sentence)\n",
    "print(f\"Response: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
